# Implementation of NeuralNetwork from scratch using Relu and Softmax activation functions
To execute the code enter : 

python3 Main.py -test_percent 20 -path circles.txt -sizes 2 6 2 -lrate 0.001 -epochs 1000

sizes : 
dimension of the input,
number of neurones per hidden layer, 
number of output neurones.

lrate is the learning rate
